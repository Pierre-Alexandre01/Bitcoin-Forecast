# -*- coding: utf-8 -*-
"""Bitcoin_Forecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16U15iYq5ECX8U_Na5f_fv2uGKPhplLsN
"""

# ============================================================
# Colab Notebook: Bitcoin Sentiment ML (FinBERT + yfinance)
# ============================================================

# ---------- 0) Setup & Installs ----------
!pip -q install snscrape transformers torch --extra-index-url https://download.pytorch.org/whl/cpu
!pip -q install yfinance scikit-learn pandas numpy matplotlib emoji nltk joblib

import os, re, math, json, time, warnings, emoji, numpy as np, pandas as pd, matplotlib.pyplot as plt
from datetime import datetime, timedelta, timezone

warnings.filterwarnings("ignore")
plt.rcParams["figure.figsize"] = (9,5)
plt.rcParams["axes.grid"] = True

# ---------- 0.1) Configuration ----------
USE_FINBERT = True        # Set False to use VADER (faster, lighter)
OFFLINE_MODE = False      # Set True to upload your own CSV of texts
SAVE_TO_DRIVE = False     # Set True to save artifacts to your Google Drive
SCRAPE_DAYS = 30           # How many days back to scrape
SCRAPE_MAX = 2500         # Max posts to collect
QUERY = "(bitcoin OR btc) lang:en"  # snscrape query

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# ---------- 0.2) Optional: Mount Drive ----------
drive_base = ""
if SAVE_TO_DRIVE:
    from google.colab import drive
    drive.mount("/content/drive", force_remount=True)
    drive_base = "/content/drive/MyDrive/bitcoin_sentiment_artifacts"
    os.makedirs(drive_base, exist_ok=True)
else:
    drive_base = "/content/bitcoin_sentiment_artifacts"
    os.makedirs(drive_base, exist_ok=True)

# ---------- 1) Imports for Sentiment & Data ----------
# NLTK (VADER) optional
VADER_AVAILABLE = True
try:
    import nltk
    nltk.download("vader_lexicon", quiet=True)
    from nltk.sentiment import SentimentIntensityAnalyzer
except Exception:
    VADER_AVAILABLE = False

# FinBERT
if USE_FINBERT:
    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Prices
import yfinance as yf

# ---------- 2) Helpers ----------
URL_RE = re.compile(r"http\S+|www\.\S+")
HANDLE_RE = re.compile(r"@[A-Za-z0-9_]+")
HASHTAG_RE = re.compile(r"#")
MULTISPACE_RE = re.compile(r"\s+")
RT_RE = re.compile(r"\bRT\b:?")

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = RT_RE.sub("", s)
    s = URL_RE.sub("", s)
    s = HANDLE_RE.sub("@user", s)
    s = HASHTAG_RE.sub("", s)  # keep the word, drop '#'
    s = emoji.replace_emoji(s, replace="")
    s = s.strip()
    s = MULTISPACE_RE.sub(" ", s)
    return s

def softmax_np(x):
    e = np.exp(x - np.max(x))
    return e / e.sum()

def show_head(df, n=5, title=None):
    if title: print(f"\n=== {title} ===")
    display(df.head(n))

# ---------- 3) Collect Bitcoin-related posts (Twitter/News/Reddit/TikTok) ----------
# Safe installs (split so one failure doesn't kill the rest)
!pip -q install -U requests urllib3 certifi feedparser gnews
# snscrape often breaks on Colab; install if available, but don't fail if not
!pip -q install snscrape==0.4.0.20230107 || true

import os, json, time, certifi, requests, pandas as pd, feedparser
from datetime import datetime, timedelta, timezone

os.environ["SSL_CERT_FILE"] = certifi.where()

# -------- Settings --------
QUERY          = "(bitcoin OR btc) lang:en"
SCRAPE_DAYS    = 7
SCRAPE_MAX     = 1200

USE_TWITTER    = True      # tries snscrape; falls back automatically if blocked
USE_GOOGLE_NEWS= True
USE_REDDIT     = True      # uses subreddit RSS (no keys)
USE_TIKTOK     = True      # requires APIFY_TOKEN; set below

APIFY_TOKEN    = ""        # <- put your Apify token here if you want TikTok
TIKTOK_QUERY   = "bitcoin" # what to search on TikTok
TIKTOK_LIMIT   = 100

# -------- Utilities --------
def _to_utc(ts):
    try:
        return pd.to_datetime(ts, utc=True)
    except Exception:
        return pd.Timestamp.utcnow()

def _clean_rows(rows):
    df = pd.DataFrame(rows)
    if not df.empty:
        df["date"] = pd.to_datetime(df["date"], utc=True)
        df["text"] = df["text"].fillna("").astype(str).str.strip()
        df = df[df["text"].str.len() > 0]
    return df

# -------- Twitter (best-effort) --------
def scrape_tweets_python(query=QUERY, days=SCRAPE_DAYS, max_results=SCRAPE_MAX):
    try:
        import snscrape.modules.twitter as sntwitter  # noqa
    except Exception as e:
        raise RuntimeError(f"snscrape python unavailable: {e}")

    since = (datetime.now(timezone.utc) - timedelta(days=days)).strftime("%Y-%m-%d")
    search = f"{query} since:{since}"
    print(f"[twitter:python] {search}")

    rows = []
    for i, tw in enumerate(sntwitter.TwitterSearchScraper(search).get_items()):
        if i >= max_results: break
        if getattr(tw, "retweetedTweet", None) is not None:  # skip RTs
            continue
        rows.append({"date": pd.to_datetime(tw.date).tz_convert("UTC"),
                     "text": tw.rawContent or "", "source": "twitter"})
    if not rows:
        raise RuntimeError("twitter python returned 0 rows")
    return _clean_rows(rows)

def scrape_tweets_cli(query=QUERY, days=SCRAPE_DAYS, max_results=SCRAPE_MAX):
    since = (datetime.now(timezone.utc) - timedelta(days=days)).strftime("%Y-%m-%d")
    search = f'twitter-search "{query} since:{since}"'
    out_path = "/content/tweets.jsonl"
    print(f"[twitter:cli] {search}")
    rc = os.system(f"snscrape --jsonl --max-results {max_results} {search} > {out_path} 2>/dev/null")
    if rc != 0 or not os.path.exists(out_path):
        raise RuntimeError("snscrape CLI failed or unavailable")
    rows = []
    with open(out_path, "r") as f:
        for line in f:
            o = json.loads(line)
            rows.append({"date": _to_utc(o.get("date")),
                         "text": (o.get("rawContent") or ""), "source": "twitter"})
    if not rows: raise RuntimeError("twitter cli returned 0 rows")
    return _clean_rows(rows)

def get_twitter():
    if not USE_TWITTER:
        return pd.DataFrame(columns=["date","text","source"])
    # two attempts with python, then CLI
    for k in range(2):
        try:
            return scrape_tweets_python()
        except Exception as e:
            print(f"[twitter:python] attempt {k+1}/2 failed -> {e}")
            time.sleep(6*(k+1))
    try:
        return scrape_tweets_cli()
    except Exception as e:
        print(f"[twitter:cli] failed -> {e}")
        return pd.DataFrame(columns=["date","text","source"])

# -------- Google News --------
def get_google_news(topic="Bitcoin", max_results=300):
    if not USE_GOOGLE_NEWS:
        return pd.DataFrame(columns=["date","text","source"])
    from gnews import GNews
    g = GNews(language="en", max_results=max_results)
    arts = g.get_news(topic)
    rows = []
    for a in arts:
        dt = a.get("published date") or a.get("publishedDate") or datetime.utcnow()
        title = a.get("title") or ""
        desc  = a.get("description") or a.get("summary") or ""
        rows.append({"date": _to_utc(dt), "text": f"{title}. {desc}".strip(), "source": "google_news"})
    print(f"[google_news] {len(rows)} items")
    return _clean_rows(rows)

# -------- Reddit via RSS (no API keys) --------
def get_reddit_rss(subreddits=("Bitcoin","CryptoCurrency","BitcoinMarkets","CryptoMarkets"),
                   days=SCRAPE_DAYS, max_per_sub=300):
    if not USE_REDDIT:
        return pd.DataFrame(columns=["date","text","source"])
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    rows = []
    for sub in subreddits:
        url = f"https://www.reddit.com/r/{sub}/.rss"
        try:
            feed = feedparser.parse(url)
            cnt = 0
            for e in feed.entries:
                if cnt >= max_per_sub: break
                dt = _to_utc(getattr(e, "published", getattr(e, "updated", datetime.utcnow())))
                if dt.tz_convert("UTC") < pd.Timestamp(cutoff):
                    continue
                title = getattr(e, "title", "")
                summary = getattr(e, "summary", "")
                txt = f"{title}. {summary}".strip()
                if len(txt) < 6: continue
                rows.append({"date": dt, "text": txt, "source": f"reddit/r/{sub}"})
                cnt += 1
            print(f"[reddit:r/{sub}] {cnt} items")
        except Exception as ex:
            print(f"[reddit:r/{sub}] failed -> {ex}")
    return _clean_rows(rows)

# -------- TikTok via Apify (requires token) --------
def get_tiktok_via_apify(query=TIKTOK_QUERY, limit=TIKTOK_LIMIT, token=APIFY_TOKEN):
    if not USE_TIKTOK or not token:
        print("[tiktok] Skipped (no APIFY_TOKEN)")
        return pd.DataFrame(columns=["date","text","source"])
    endpoint = f"https://api.apify.com/v2/acts/apify~tiktok-scraper/run-sync?token={token}"
    payload = {
        "search": query,
        "resultsPerPage": 50,
        "maxResults": int(limit),
        "downloadVideos": False,
        "includeComments": False
    }
    print("[tiktok] querying Apify…")
    try:
        r = requests.post(endpoint, json=payload, timeout=120)
        r.raise_for_status()
        out = r.json()
        rows = []
        for item in out.get("dataset", out).get("items", out.get("items", [])):
            # fallback parsing across Apify versions
            txt = item.get("desc") or item.get("text") or ""
            created = item.get("createTime") or item.get("createdAt") or datetime.utcnow().isoformat()
            if isinstance(created, (int, float)):
                dt = pd.to_datetime(created, unit="s", utc=True)
            else:
                dt = _to_utc(created)
            rows.append({"date": dt, "text": txt, "source": "tiktok"})
        print(f"[tiktok] {len(rows)} items")
        return _clean_rows(rows)
    except Exception as e:
        print(f"[tiktok] failed -> {e}")
        return pd.DataFrame(columns=["date","text","source"])

# -------- Pull all sources & unify --------
frames = []
tw  = get_twitter()
gn  = get_google_news("Bitcoin", max_results=400)
rd  = get_reddit_rss()
tt  = get_tiktok_via_apify()

for df_ in (tw, gn, rd, tt):
    if not df_.empty:
        frames.append(df_)

if not frames:
    raise RuntimeError("No data from any source. Consider OFFLINE_MODE or providing an API token.")

df_posts = pd.concat(frames, ignore_index=True)
# De-duplicate identical texts across sources
df_posts = df_posts.drop_duplicates(subset=["text"]).sort_values("date").reset_index(drop=True)

print(f"\nCombined rows: {len(df_posts)} "
      f"(twitter={len(tw)}, news={len(gn)}, reddit={len(rd)}, tiktok={len(tt)})")

show_head(df_posts, title="Sample posts (Twitter + Google News + Reddit + TikTok)")

# ---------- 4) Sentiment Scoring (robust) ----------
# This cell works even if 'text_clean' wasn't created yet.

# Ensure clean_text exists (no-op fallback if user removed earlier cell)
try:
    clean_text
except NameError:
    import re, emoji
    URL_RE = re.compile(r"http\S+|www\.\S+")
    HANDLE_RE = re.compile(r"@[A-Za-z0-9_]+")
    HASHTAG_RE = re.compile(r"#")
    MULTISPACE_RE = re.compile(r"\s+")
    RT_RE = re.compile(r"\bRT\b:?")
    def clean_text(s: str) -> str:
        if not isinstance(s, str): return ""
        s = RT_RE.sub("", s)
        s = URL_RE.sub("", s)
        s = HANDLE_RE.sub("@user", s)
        s = HASHTAG_RE.sub("", s)
        s = emoji.replace_emoji(s, replace="")
        return MULTISPACE_RE.sub(" ", s).strip()

# 0) Make sure we have a clean text column
if "text_clean" not in df_posts.columns:
    df_posts["text_clean"] = df_posts["text"].astype(str).apply(clean_text)

# Filter out very short/empty rows
df_posts = df_posts[df_posts["text_clean"].str.len() > 5].reset_index(drop=True)

# Quick sanity check
if df_posts.empty:
    raise RuntimeError("No rows to score. Upstream collectors produced no usable text.")

print("\nScoring sentiment...")

# --- FinBERT / VADER helpers ---
def load_finbert(model_name="ProsusAI/finbert"):
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    model.eval()
    return tokenizer, model



def finbert_score(texts, batch_size=32, device=None):
    import torch, numpy as np, pandas as pd
    tokenizer, model = load_finbert()
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    def softmax_np(x):
        e = np.exp(x - np.max(x))
        return e / e.sum()

    out = []
    labels = ["negative", "neutral", "positive"]
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            enc = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors="pt").to(device)
            logits = model(**enc).logits.detach().cpu().numpy()
            probs = np.apply_along_axis(softmax_np, 1, logits)
            for p in probs:
                out.append({
                    "neg": float(p[0]),
                    "neu": float(p[1]),
                    "pos": float(p[2]),
                    "sentiment": labels[int(np.argmax(p))]
                })
    return pd.DataFrame(out)

def vader_score(texts):
    import pandas as pd
    from nltk.sentiment import SentimentIntensityAnalyzer
    sia = SentimentIntensityAnalyzer()
    rows = []
    for t in texts:
        s = sia.polarity_scores(t)
        if s["compound"] > 0.05:
            lab = "positive"
        elif s["compound"] < -0.05:
            lab = "negative"
        else:
            lab = "neutral"
        rows.append({"neg": s["neg"], "neu": s["neu"], "pos": s["pos"], "sentiment": lab})
    return pd.DataFrame(rows)

# 1) Score
texts = df_posts["text_clean"].tolist()
BATCH = 64 if (USE_FINBERT and "torch" in globals() and torch.cuda.is_available()) else 32
if USE_FINBERT:
    df_scores = finbert_score(texts, batch_size=BATCH)
else:
    df_scores = vader_score(texts)

# 2) Join back and add UTC column
df = pd.concat([df_posts.reset_index(drop=True), df_scores.reset_index(drop=True)], axis=1)
df["date_utc"] = pd.to_datetime(df["date"]).dt.tz_convert("UTC") if hasattr(df["date"].iloc[0], "tzinfo") else pd.to_datetime(df["date"], utc=True)

show_head(df, title="Scored posts")

# ---------- 5) Aggregate Daily Features (clean + safe join) ----------
def daily_features(df_in):
    d = df_in.copy()
    d["date_day"] = d["date_utc"].dt.floor("D")
    d["is_pos"] = (d["sentiment"] == "positive").astype(int)
    d["is_neu"] = (d["sentiment"] == "neutral").astype(int)
    d["is_neg"] = (d["sentiment"] == "negative").astype(int)
    agg = (d.groupby("date_day")
             .agg(n=("text","count"),
                  pos_share=("is_pos","mean"),
                  neg_share=("is_neg","mean"),
                  neu_share=("is_neu","mean"),
                  pos_mean=("pos","mean"),
                  neg_mean=("neg","mean"),
                  neu_mean=("neu","mean"))
             .reset_index())
    agg["sent_balance"] = agg["pos_share"] - agg["neg_share"]
    return agg

# 1) Rebuild from scratch each run
df_daily = daily_features(df).sort_values("date_day")

# 2) Shift (no look-ahead; yesterday's sentiment only)
for c in ["pos_share","neg_share","neu_share","pos_mean","neg_mean","neu_mean","sent_balance"]:
    df_daily[c] = df_daily[c].shift(1)

# 3) Per-source sentiment shares
tmp = df.copy()
tmp["date_day"] = tmp["date_utc"].dt.floor("D")
tmp["is_pos"] = (tmp["sentiment"] == "positive").astype(int)
tmp["is_neg"] = (tmp["sentiment"] == "negative").astype(int)

by_src = (tmp.groupby(["date_day","source"])
            .agg(n_src=("text","count"),
                 pos_share_src=("is_pos","mean"),
                 neg_share_src=("is_neg","mean"))
            .reset_index())

# make safe, consistent column names
by_src["source_safe"] = by_src["source"].astype(str).str.replace(r"[^A-Za-z0-9]+", "_", regex=True)

pivot_pos = by_src.pivot(index="date_day", columns="source_safe", values="pos_share_src")
pivot_pos.columns = [f"pos_{c}" for c in pivot_pos.columns]
pivot_pos = pivot_pos.loc[:, ~pivot_pos.columns.duplicated()]

pivot_neg = by_src.pivot(index="date_day", columns="source_safe", values="neg_share_src")
pivot_neg.columns = [f"neg_{c}" for c in pivot_neg.columns]
pivot_neg = pivot_neg.loc[:, ~pivot_neg.columns.duplicated()]

# 4) Drop any columns that already exist in df_daily to avoid overlap errors
cols_existing = set(df_daily.columns)
pivot_pos = pivot_pos[[c for c in pivot_pos.columns if c not in cols_existing]]
cols_existing |= set(pivot_pos.columns)  # update after first join
pivot_neg = pivot_neg[[c for c in pivot_neg.columns if c not in cols_existing]]

# 5) Join one-by-one
df_daily = (df_daily.set_index("date_day")
                    .join(pivot_pos, how="left")
                    .join(pivot_neg, how="left")
                    .reset_index())

# 6) Shift per-source columns too (no look-ahead) — exclude base columns
base_cols = {"pos_share","neg_share","neu_share","pos_mean","neg_mean","neu_mean","sent_balance"}
for c in df_daily.columns:
    if (c.startswith(("pos_", "neg_"))) and (c not in base_cols):
        df_daily[c] = df_daily[c].shift(1)

# ---------- 6) Get BTC Prices & Join (super‑robust + safe timezone) ----------
from datetime import datetime, timedelta

def ensure_utc(series):
    """Convert a pandas datetime Series to UTC safely (tz‑naive or tz‑aware)."""
    s = pd.to_datetime(series, errors="coerce")
    try:
        tz = s.dt.tz
    except Exception:
        tz = None
    if tz is None:
        s = s.dt.tz_localize("UTC")
    else:
        s = s.dt.tz_convert("UTC")
    return s.dt.floor("D")

def load_btc_prices(start, end, interval="1d"):
    """Return clean BTC-USD with date_day, Open/High/Low/Close/Adj Close/Volume."""
    tkr = yf.Ticker("BTC-USD")
    px = tkr.history(start=start.date(), end=end.date(), interval=interval, auto_adjust=False)

    if px is None or px.empty:
        print("[yfinance] Empty result, widening window to 90d…")
        px = tkr.history(period="90d", interval=interval, auto_adjust=False)

    # Flatten MultiIndex if present
    if isinstance(px.columns, pd.MultiIndex):
        px.columns = ["_".join([c for c in col if c]).strip() for col in px.columns.values]

    px = px.reset_index().rename(columns={"Date": "date_day"})

    # Normalize column names (case-insensitive)
    cols = {c.lower(): c for c in px.columns}
    def getcol(name, fallback=None):
        key = name.lower()
        return cols.get(key, fallback)

    # Ensure core columns exist (fall back / rename if needed)
    for want in ["Open","High","Low","Close","Volume"]:
        c = getcol(want)
        if c is None and want == "Volume":
            # Crypto sometimes lacks Volume; create a placeholder
            px["Volume"] = np.nan
        elif c is None:
            raise KeyError(f"Price column '{want}' not found in yfinance output: {list(px.columns)}")
        elif c != want:
            px[want] = px[c]

    # Adj Close (fallback to Close)
    adj_c = getcol("Adj Close")
    if adj_c is None:
        px["Adj Close"] = px["Close"]
    elif adj_c != "Adj Close":
        px["Adj Close"] = px[adj_c]

    keep = ["date_day","Open","High","Low","Close","Adj Close"]
    if "Volume" in px.columns:
        keep.append("Volume")
    px = px[keep]

    # TZ & returns
    px["date_day"] = ensure_utc(px["date_day"])
    px["ret"] = px["Adj Close"].pct_change()
    px["ret_next"] = px["ret"].shift(-1)
    px["up_next"] = (px["ret_next"] > 0).astype(int)
    return px

end = datetime.utcnow()
start = end - timedelta(days=max(30, SCRAPE_DAYS + 7))
px = load_btc_prices(start, end, interval="1d")

# Ensure df_daily key matches (safe timezone conversion)
df_daily["date_day"] = ensure_utc(df_daily["date_day"])

# Join price + sentiment
data = px.merge(df_daily, on="date_day", how="left").sort_values("date_day").reset_index(drop=True)

# Forward-fill sentiment for sparse days
for col in ["n","pos_share","neg_share","neu_share","pos_mean","neg_mean","neu_mean","sent_balance"]:
    if col in data.columns:
        data[col] = data[col].fillna(method="ffill", limit=3)  # cap to 3 days

show_head(data, title="Joined price + sentiment")

# ---------- 7) EDA ----------
print("\nPlotting BTC vs. Sentiment balance...")
fig, ax = plt.subplots()

# Primary axis for BTC price
ax.plot(
    data["date_day"], data["Adj Close"],
    color="tab:blue", label="BTC Adj Close", linewidth=2
)

# Secondary axis for sentiment
ax2 = ax.twinx()
ax2.plot(
    data["date_day"], data["sent_balance"],
    color="tab:orange", label="Sentiment Balance (pos - neg)", linewidth=2, linestyle="--"
)

ax.set_title("BTC Price vs. Daily Sentiment Balance")
ax.set_xlabel("Date")
ax.set_ylabel("Price (USD)", color="tab:blue")
ax2.set_ylabel("Sentiment balance", color="tab:orange")

# Legends for both axes
lines1, labels1 = ax.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
fig.legend(lines1 + lines2, labels1 + labels2, loc="upper left")

import matplotlib.dates as mdates
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
fig.autofmt_xdate()
plt.tight_layout()
plt.show()

# Correlation calculation
corr = np.corrcoef(data["ret"].fillna(0), data["sent_balance"].fillna(0))[0, 1]
print(f"Correlation(ret, sentiment balance) = {corr:.3f}")

# ---------- 8) Simple Predictive Model (robust) ----------
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier

# Same features as before
features = ["pos_share","neg_share","neu_share","pos_mean","neg_mean","neu_mean","sent_balance","ret"]

# Keep rows with a target and valid features
model_df = data.dropna(subset=["up_next"]).copy()
model_df[features] = model_df[features].replace([np.inf, -np.inf], np.nan)
model_df = model_df.dropna(subset=features)

X = model_df[features].to_numpy()
y = model_df["up_next"].astype(int).to_numpy()

# Pipeline
pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=1000))
])

# Choose splits based on sample size (avoid tiny folds)
max_splits = min(4, max(2, len(model_df) // 7))

scores = []
if len(np.unique(y)) >= 2 and max_splits >= 2:
    tscv = TimeSeriesSplit(n_splits=max_splits)
    for fold, (tr, te) in enumerate(tscv.split(X), start=1):
        y_tr = y[tr]
        # Skip folds where the training set has a single class
        if len(np.unique(y_tr)) < 2:
            print(f"[CV] Skipping fold {fold}: training set has one class.")
            continue
        pipe.fit(X[tr], y_tr)
        y_pred = pipe.predict(X[te])
        acc = accuracy_score(y[te], y_pred)
        scores.append(acc)
    if scores:
        print("TimeSeries CV accuracy:", np.round(scores, 3), "mean:", np.mean(scores).round(3))
    else:
        print("Not enough class variety for CV; fitting on all data.")
else:
    print("Dataset too small or only one class overall; fitting without CV.")

# If overall target has only one class, fall back to a baseline model
if len(np.unique(y)) < 2:
    pipe = Pipeline([("scaler", StandardScaler()),
                     ("clf", DummyClassifier(strategy="most_frequent"))])

# Fit on all data and show in-sample probabilities
pipe.fit(X, y)
if hasattr(pipe.named_steps["clf"], "predict_proba"):
    model_df["proba_up"] = pipe.predict_proba(X)[:, 1]
else:
    model_df["proba_up"] = np.full(len(X), y.mean())

show_head(model_df[["date_day","up_next","proba_up"]].tail(10), title="Model outputs (in-sample)")

# ---------- 8b) Add lags/rolls + re-train (adaptive) ----------
fe = data.copy()

# always avoid look-ahead
fe["sent_lag1"]  = fe["sent_balance"].shift(1)
fe["ret_lag1"]   = fe["ret"].shift(1)

# only build longer-history features; they may become NaN if sample is tiny
fe["sent_lag3"]  = fe["sent_balance"].shift(3)
fe["ret_lag3"]   = fe["ret"].shift(3)
fe["sent_roll3"] = fe["sent_balance"].shift(1).rolling(3, min_periods=2).mean()
fe["sent_roll7"] = fe["sent_balance"].shift(1).rolling(7, min_periods=3).mean()

target = "up_next"

# Choose features adaptively based on available rows
n_rows = len(fe.dropna(subset=["up_next"]))
if n_rows < 12:
    features = ["sent_lag1", "ret_lag1"]                     # minimal set
elif n_rows < 25:
    features = ["sent_lag1", "ret_lag1", "sent_roll3"]       # add a short roll
else:
    features = ["sent_lag1","sent_lag3","sent_roll3","sent_roll7","ret_lag1","ret_lag3"]

# Build modeling frame and clean
model_df2 = fe.replace([np.inf, -np.inf], np.nan).dropna(subset=features + [target]).copy()

if len(model_df2) < 3:
    print(f"Not enough rows after lags/rolls (got {len(model_df2)}). "
          "Increase SCRAPE_DAYS (try 30–60) or run again later to collect more posts.")
else:
    X = model_df2[features].to_numpy()
    y = model_df2[target].astype(int).to_numpy()

    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score

    pipe = Pipeline([("scaler", StandardScaler()),
                     ("clf", LogisticRegression(max_iter=1000))])

    # Time-series CV with safe split count
    n_splits = max(2, min(4, len(model_df2) - 1))
    scores = []
    try:
        tscv = TimeSeriesSplit(n_splits=n_splits)
        for fold, (tr, te) in enumerate(tscv.split(X), 1):
            if len(np.unique(y[tr])) < 2:
                print(f"[CV] Skip fold {fold} (one class in train)")
                continue
            pipe.fit(X[tr], y[tr])
            pred = pipe.predict(X[te])
            scores.append(accuracy_score(y[te], pred))
        if scores:
            print(f"TS CV accuracy ({features}):", np.round(scores, 3), "mean:", np.mean(scores).round(3))
        else:
            print("No valid CV folds; fitting on full sample.")
    except ValueError as e:
        print(f"[CV] Skipped due to small sample: {e}")

    # Fit on all rows and show last few probabilities
    pipe.fit(X, y)
    model_df2["proba_up"] = pipe.predict_proba(X)[:, 1]
    show_head(model_df2[["date_day","up_next","proba_up"]].tail(10), title="Model outputs (adaptive lags)")

# ---------- 8c) Walk-forward holdout (NaN-safe) ----------
use_cols = ["sent_balance","ret","pos_share","neg_share","neu_share","pos_google_news","neg_google_news"]
use_cols = [c for c in use_cols if c in data.columns]

fe = data.copy()
for c in [x for x in use_cols if x != "ret"]:
    fe[f"{c}_lag1"]  = fe[c].shift(1)
    fe[f"{c}_roll3"] = fe[c].shift(1).rolling(3, min_periods=3).mean()
fe["ret_lag1"] = fe["ret"].shift(1)

# keep rows that have a target
fe = fe.dropna(subset=["up_next"]).copy()

# candidate feature list
features_wf = [c for c in fe.columns if c.endswith("_lag1") or c.endswith("_roll3") or c == "ret_lag1"]
if not features_wf:
    raise RuntimeError("No walk-forward features were created. Check upstream feature names.")

# 80/20 split (time-ordered)
split = max(1, int(len(fe) * 0.8))
train, test = fe.iloc[:split].copy(), fe.iloc[split:].copy()
if len(test) == 0:
    raise RuntimeError("Holdout set is empty. Increase your window (SCRAPE_DAYS) to create a test segment.")

# drop feature columns that are all-NaN in the TRAIN window
features_wf = [c for c in features_wf if train[c].notna().any()]
if not features_wf:
    raise RuntimeError("All candidate features are NaN in the train window. Increase SCRAPE_DAYS or reduce lags.")

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# drop columns that are all-NaN in TEST too
features_wf = [c for c in features_wf if test[c].notna().any()]

# Build X/y
Xtr, ytr = train[features_wf].to_numpy(), train["up_next"].astype(int).to_numpy()
Xte, yte = test[features_wf].to_numpy(),  test["up_next"].astype(int).to_numpy()

# Pipeline with imputer -> scaler -> model
pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),   # fit on TRAIN only
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=1000))
])

# If training target has one class, fall back to a dummy baseline
if len(np.unique(ytr)) < 2:
    print("Train set has one class; using DummyClassifier (most_frequent).")
    pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="mean")),
        ("scaler", StandardScaler(with_mean=False)),  # scaler not needed, keep safe
        ("clf", DummyClassifier(strategy="most_frequent"))
    ])

# Fit and evaluate
pipe.fit(Xtr, ytr)
pipe_holdout = pipe  # keep reference to the fitted holdout model
pred  = pipe.predict(Xte)
proba = pipe.predict_proba(Xte)[:, 1] if hasattr(pipe.named_steps["clf"], "predict_proba") \
        else np.full(len(Xte), ytr.mean() if len(ytr) else 0.5)

acc = accuracy_score(yte, pred)
print(f"Holdout accuracy: {acc:.3f}")

show_head(
    pd.DataFrame({"date_day": test["date_day"], "up_next": yte, "proba_up": proba}).tail(10),
    title="Holdout predictions (NaN-safe)"
)

# ---------- 8d) Improved rolling walk-forward backtest (HGB + tuned threshold) ----------
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# 1) Build richer yesterday-only features (no look-ahead)
fe = data.copy()

# Base sentiment & price
fe["sent_lag1"]  = fe["sent_balance"].shift(1)
fe["sent_lag3"]  = fe["sent_balance"].shift(3)
fe["sent_r3"]    = fe["sent_balance"].shift(1).rolling(3, min_periods=2).mean()
fe["sent_r7"]    = fe["sent_balance"].shift(1).rolling(7, min_periods=3).mean()
fe["ret_lag1"]   = fe["ret"].shift(1)
fe["ret_lag3"]   = fe["ret"].shift(3)
fe["vol_5"]      = fe["ret"].shift(1).rolling(5, min_periods=3).std()
fe["vol_10"]     = fe["ret"].shift(1).rolling(10, min_periods=5).std()

# Per-source balances if present
for c in [col for col in fe.columns if col.startswith("pos_")]:
    neg_c = "neg_" + c[4:]
    if neg_c in fe.columns:
        bal = fe[c] - fe[neg_c]
        fe[c.replace("pos_", "srcbal_") + "_lag1"] = bal.shift(1)
        fe[c.replace("pos_", "srcbal_") + "_r3"]   = bal.shift(1).rolling(3, min_periods=2).mean()

# Keep rows with a target
fe = fe.dropna(subset=["up_next"]).copy()

# Feature list
feat_cols = [c for c in fe.columns if any(c.startswith(prefix) for prefix in
             ("sent_", "ret_", "vol_", "srcbal_"))]

if len(fe) < 10 or not feat_cols:
    print("Backtest: not enough rows/features. Increase SCRAPE_DAYS.")
else:
    start_train = max(12, min(40, len(fe) // 2))  # expanding window start
    preds, rets, dates, ups, thrs = [], [], [], [], []

    for i in range(start_train, len(fe) - 1):
        train = fe.iloc[:i]
        test1 = fe.iloc[i:i+1]

        # Drop all-NaN cols in train
        keep_cols = [c for c in feat_cols if train[c].notna().any()]
        if not keep_cols or len(np.unique(train["up_next"])) < 2:
            preds.append(0.5)
            rets.append(float(test1["ret_next"].fillna(0)))
            dates.append(test1["date_day"].iloc[0])
            ups.append(int(test1["up_next"]))
            thrs.append(0.5)
            continue

        # Train/val split (last 20% of train = val)
        split_idx = max(1, int(len(train) * 0.8))
        trX, trY = train[keep_cols].iloc[:split_idx], train["up_next"].iloc[:split_idx]
        valX, valY = train[keep_cols].iloc[split_idx:], train["up_next"].iloc[split_idx:]

        # Impute
        imp = SimpleImputer(strategy="mean")
        trX = imp.fit_transform(trX)
        valX = imp.transform(valX)

        # Model
        clf = HistGradientBoostingClassifier(
            max_depth=3, learning_rate=0.08, max_iter=200,
            min_samples_leaf=5, random_state=42
        )
        clf.fit(trX, trY)

        # Tune threshold on validation
        p_val = clf.predict_proba(valX)[:, 1]
        # align validation returns to p_val window
        ret_val = train["ret_next"].iloc[split_idx:].to_numpy()
        grid = np.linspace(0.45, 0.65, 21)
        # maximize equity growth on validation
        eq = [np.prod(1.0 + (p_val > t) * ret_val) for t in grid]
        best_thr = grid[int(np.argmax(eq))]
        thrs.append(best_thr)

        # Predict next day
        p_next = clf.predict_proba(imp.transform(test1[keep_cols]))[:, 1][0]
        preds.append(float(p_next))
        rets.append(float(test1["ret_next"].fillna(0)))
        dates.append(test1["date_day"].iloc[0])
        ups.append(int(test1["up_next"]))

    # Assemble results
    fe_bt = pd.DataFrame({
        "date_day": dates,
        "proba_up": preds,
        "ret_next": rets,
        "up_next": ups,
        "thr": thrs
    })
    # optional probability smoothing
    fe_bt["p_smooth"] = fe_bt["proba_up"].ewm(alpha=0.3, adjust=False).mean()

    # no-trade band: skip when too close to 0.5
    band = 0.03  # tune by validation equity
    sig_raw = (fe_bt["p_smooth"] > fe_bt["thr"]).astype(int)
    neutral = (fe_bt["p_smooth"].sub(0.5).abs() < band)
    fe_bt["sig"] = np.where(neutral, 0, sig_raw)

    fe_bt["equity"] = (1 + fe_bt["sig"] * fe_bt["ret_next"]).cumprod()
    bh = (1 + fe_bt["ret_next"]).cumprod()

    # Plot
    plt.figure(figsize=(10,5))
    plt.plot(fe_bt["date_day"], fe_bt["equity"], label="Strategy (tuned thr)")
    plt.plot(fe_bt["date_day"], bh, label="Buy & Hold", alpha=0.7)
    plt.legend()
    plt.title("Rolling WF Backtest (Gradient Boosting + Tuned Threshold)")
    plt.tight_layout()
    plt.show()

    acc = accuracy_score(fe_bt["up_next"], fe_bt["sig"])
    print(f"Backtest accuracy: {acc:.3f} over {len(fe_bt)} predictions")
    print(f"Final equity: {fe_bt['equity'].iloc[-1]:.3f} | B&H: {bh.iloc[-1]:.3f}")
    print(f"Avg tuned threshold: {np.mean(fe_bt['thr']):.3f}")

# === Long intraday history builder (multi-exchange via ccxt) ===
!pip -q install ccxt > /dev/null

import ccxt, pandas as pd, numpy as np, time, os
from datetime import datetime, timezone

# --- config ---
SYMBOLS = {
    "kraken":   ("BTC/USD",  "1h",  "5m"),
    "bitfinex": ("BTC/USD",  "1h",  "5m"),  # ✅ fixed
    "okx":      ("BTC/USDT", "1h",  "5m"),
    "bybit":    ("BTC/USDT", "1h",  "5m"),
    "kucoin":   ("BTC/USDT", "1h",  "5m"),
    "coinbase": ("BTC/USD",  "1h",  None),
}
SINCE_ISO = "2019-01-01T00:00:00Z"  # go as far back as the exchange allows
SAVE_DIR  = "/content/crypto_hist"  # change as you like

os.makedirs(SAVE_DIR, exist_ok=True)

def fetch_ohlcv_full(ex_id, symbol, tf, since_iso, limit=1000, max_loops=1000, pause=0.5):
    """Forward paginate until exhaustion. Returns DataFrame with dt, open, high, low, close, volume."""
    ex = getattr(ccxt, ex_id)()
    since = ex.parse8601(since_iso)
    all_rows = []
    loops = 0
    while True:
        loops += 1
        try:
            rows = ex.fetch_ohlcv(symbol, timeframe=tf, since=since, limit=limit)
        except Exception as e:
            print(f"[{ex_id}:{tf}] error -> {e}")
            break
        if not rows:
            break
        all_rows.extend(rows)
        # advance 'since' to the next candle
        step_ms = ex.parse_timeframe(tf) * 1000
        since = rows[-1][0] + step_ms
        if len(rows) < limit or loops >= max_loops:
            break
        if pause: time.sleep(pause)

    if not all_rows:
        return pd.DataFrame()

    df = pd.DataFrame(all_rows, columns=["ms","open","high","low","close","volume"])
    df["dt"] = pd.to_datetime(df["ms"], unit="ms", utc=True)
    df = df.drop_duplicates(subset=["dt"]).sort_values("dt")
    df["exchange"] = ex_id
    df["symbol"]   = symbol
    return df

def try_exchange(ex_id, pair, pref_tf, fb_tf, since_iso):
    """Fetch preferred tf; if too short or empty, try fallback; resample to 1H if needed."""
    print(f"→ {ex_id} {pair} (pref={pref_tf}, fallback={fb_tf})")
    df = fetch_ohlcv_full(ex_id, pair, pref_tf, since_iso)
    used = pref_tf
    # If preferred timeframe failed or is short, try fallback
    if (df.empty or df["dt"].nunique() < 200) and fb_tf:
        print(f"   …trying fallback {fb_tf}")
        fb = fetch_ohlcv_full(ex_id, pair, fb_tf, since_iso)
        if not fb.empty:
            # resample to 1H
            fb = (fb.set_index("dt")[["open","high","low","close","volume"]]
                    .resample("1H").agg({
                        "open":"first","high":"max","low":"min","close":"last","volume":"sum"
                    }).dropna().reset_index())
            fb["exchange"], fb["symbol"] = ex_id, pair
            df = fb
            used = f"{fb_tf}->1H"
    if df.empty:
        print(f"   ✘ no data")
        return pd.DataFrame()
    print(f"   ✓ rows: {len(df)} | {df['dt'].min()} → {df['dt'].max()} | used={used}")
    return df

# --- 1) Download per-exchange & cache ---
all_raw = []
for ex_id, (pair, pref_tf, fb_tf) in SYMBOLS.items():
    out_csv = os.path.join(SAVE_DIR, f"{ex_id}_{pair.replace('/','-')}_{pref_tf}.csv")
    if os.path.exists(out_csv):
        df = pd.read_csv(out_csv, parse_dates=["dt"])
        print(f"[cache] {ex_id}: {len(df)} rows | {df['dt'].min()} → {df['dt'].max()}")
    else:
        df = try_exchange(ex_id, pair, pref_tf, fb_tf, SINCE_ISO)
        if not df.empty:
            df.to_csv(out_csv, index=False)
    if not df.empty:
        all_raw.append(df)

if not all_raw:
    raise RuntimeError("No exchange returned data. Try a VPN or adjust SYMBOLS/since.")

# --- 2) Merge: align by dt, average closes across exchanges (optional median) ---
def to_1h(df):
    # already 1H in our pipeline now, but keep function for safety
    if pd.infer_freq(df.set_index("dt").index) == "H":
        return df
    g = (df.set_index("dt")[["open","high","low","close","volume"]]
           .resample("1H").agg({"open":"first","high":"max","low":"min","close":"last","volume":"sum"})
           .dropna())
    g = g.reset_index()
    g["exchange"], g["symbol"] = df["exchange"].iloc[0], df["symbol"].iloc[0]
    return g

normed = [to_1h(x) for x in all_raw]
# wide close matrix
wide = None
for df in normed:
    name = df["exchange"].iloc[0]
    tmp = df[["dt","close"]].rename(columns={"close": f"close_{name}"})
    wide = tmp if wide is None else wide.merge(tmp, on="dt", how="outer")

wide = wide.sort_values("dt").drop_duplicates("dt")
# build a consensus close (median is robust to outliers)
close_cols = [c for c in wide.columns if c.startswith("close_")]
wide["close_cons"] = wide[close_cols].median(axis=1, skipna=True)
# fill O/H/L/V from a primary (first valid) exchange just for plotting; for RV we only need close
primary = normed[0].set_index("dt")[["open","high","low","volume"]]
merged = wide.set_index("dt").join(primary, how="left").reset_index()

print(f"Merged rows: {len(merged)} | {merged['dt'].min()} → {merged['dt'].max()} | exchanges: {len(close_cols)}")

# --- 3) Compute realized variance from consensus close ---
merged["lr2"] = np.log(merged["close_cons"]).diff().pow(2)
px_1h = merged.dropna(subset=["lr2"]).copy()
px_1h["date_day"] = px_1h["dt"].dt.floor("D")

rv_daily = (px_1h.groupby("date_day")["lr2"].sum().rename("rv").to_frame())
rv_daily["rv_next"] = rv_daily["rv"].shift(-1)

print(rv_daily.tail())

# ===== HAR-X++ intraday volatility forecasting with fallback =====
!pip -q install ccxt

import ccxt, pandas as pd, numpy as np
from datetime import datetime
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# -----------------------------
# Helper: fetch OHLCV from ccxt
# -----------------------------
def fetch_ccxt_ohlcv(ex_id, symbol, timeframe="1h", since_iso="2023-01-01T00:00:00Z", limit=1000, max_loops=50):
    ex = getattr(ccxt, ex_id)()
    since = ex.parse8601(since_iso)
    all_rows = []
    loops = 0
    while True:
        loops += 1
        try:
            ohlcv = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=limit)
        except Exception as e:
            print(f"[{ex_id}] fetch_ohlcv error:", e)
            break
        if not ohlcv:
            break
        all_rows.extend(ohlcv)
        since = ohlcv[-1][0] + ex.parse_timeframe(timeframe) * 1000
        if len(ohlcv) < limit or loops >= max_loops:
            break

    if not all_rows:
        return pd.DataFrame()

    df = pd.DataFrame(all_rows, columns=["ms","open","high","low","close","volume"])
    df["dt"] = pd.to_datetime(df["ms"], unit="ms", utc=True)
    df["date_day"] = df["dt"].dt.floor("D")
    df["exchange"] = ex_id
    df["symbol"] = symbol
    return df

# -----------------------------------
# 1) Try public exchanges in sequence
# -----------------------------------
candidates = [
    ("kraken",   "BTC/USD"),   # public OHLCV
    ("bitfinex","BTC/USD"),   # public OHLCV
    ("okx",      "BTC/USDT"),  # public OHLCV
]

px = pd.DataFrame()
for ex_id, sym in candidates:
    print(f"Trying {ex_id} {sym}…")
    tmp = fetch_ccxt_ohlcv(ex_id, sym, timeframe="1h", since_iso="2023-01-01T00:00:00Z")
    if not tmp.empty:
        px = tmp
        print(f"✔ Got {len(px)} rows from {ex_id}. Date range: {px['dt'].min()} → {px['dt'].max()}")
        break
    else:
        print(f"✘ No data from {ex_id} (or blocked).")

if px.empty:
    raise RuntimeError("Could not fetch intraday from Kraken/Bitfinex/OKX. Try another exchange or VPN.")

# ------------------------------------------
# 2) Realized variance & range from 1h bars
# ------------------------------------------
close = pd.to_numeric(px["close"], errors="coerce").ffill()
lr = np.log(close).diff()
px["lr2"] = lr.pow(2)

rv_daily = (px.groupby("date_day")["lr2"].sum()
              .rename("rv").to_frame().sort_index())
rv_daily["rv_next"] = rv_daily["rv"].shift(-1)

hi = pd.to_numeric(px["high"], errors="coerce")
lo = pd.to_numeric(px["low"],  errors="coerce")
px["logHL2"] = np.log(hi/lo).pow(2)
rv_range = px.groupby("date_day")["logHL2"].sum().rename("rv_range")

rvx = rv_daily.join(rv_range, how="left")

# -------------------------------------------------
# 3) Join sentiment data (assumes df_daily exists)
# -------------------------------------------------
# Make sure your df_daily is ready before running this block
sent = df_daily[["date_day","sent_balance","pos_mean","neg_mean","n"]].copy()
sent["date_day"] = pd.to_datetime(sent["date_day"], utc=True).dt.floor("D")
for c in ["sent_balance","pos_mean","neg_mean","n"]:
    sent[c+"_l1"] = sent[c].shift(1)
sent["sent_bal_chg_l1"] = sent["sent_balance_l1"] - sent["sent_balance_l1"].shift(1)
sent["sent_intensity_l1"] = sent["sent_balance_l1"] * (sent["n_l1"].fillna(0.0)**0.5)
sent = sent.set_index("date_day")

# ---------------------------------
# 4) HAR features + tiny-mode logic
# ---------------------------------
har = rvx.join(sent[[
    "sent_balance_l1","pos_mean_l1","neg_mean_l1","n_l1",
    "sent_bal_chg_l1","sent_intensity_l1"
]], how="left").copy()
har["y"] = np.log(har["rv_next"].clip(lower=1e-12))

if len(har) < 40:
    print(f"[Tiny-mode] Only {len(har)} rows. Using short EMAs & minimal features.")
    har["rv_ema3"]   = har["rv"].ewm(span=3, min_periods=2).mean()
    har["rv_d_lag1"] = har["rv"].shift(1)
    har["rv_w_lag1"] = har["rv_ema3"].shift(1)
    har["rv_std3_lag1"]  = har["rv"].rolling(3, min_periods=2).std().shift(1)
    X_cols = ["rv_d_lag1","rv_w_lag1","rv_std3_lag1"]
    har = har.dropna(subset=X_cols + ["y"]).copy()
    start_train = max(3, int(len(har)*0.4))
else:
    har["rv_ema5"]   = har["rv"].ewm(span=5,  min_periods=5).mean()
    har["rv_ema22"]  = har["rv"].ewm(span=22, min_periods=22).mean()
    har["rv_d_lag1"] = har["rv"].shift(1)
    har["rv_w_lag1"] = har["rv_ema5"].shift(1)
    har["rv_m_lag1"] = har["rv_ema22"].shift(1)
    har["rv_std5_lag1"]  = har["rv"].rolling(5, min_periods=5).std().shift(1)
    har["rv_jump_lag1"]  = (har["rv"].shift(1) - har["rv_ema5"].shift(1)).clip(lower=0.0)
    har["rv_range_lag1"] = har["rv_range"].shift(1)
    X_cols = ["rv_d_lag1","rv_w_lag1","rv_m_lag1",
              "rv_std5_lag1","rv_range_lag1","rv_jump_lag1",
              "sent_balance_l1","pos_mean_l1","neg_mean_l1","n_l1",
              "sent_bal_chg_l1","sent_intensity_l1"]
    har = har.dropna(subset=X_cols + ["y"]).copy()
    start_train = max(40, int(len(har)*0.3))

print("Usable rows:", len(har), "| Date range:", har.index.min(), "→", har.index.max())

# --------------------------
# 5) HAR-X++ walk-forward fit
# --------------------------
if len(har) < start_train + 1:
    raise RuntimeError(f"Not enough data to train ({len(har)} rows). Try fetching more history.")

scaler   = RobustScaler()
gb_huber = GradientBoostingRegressor(loss="huber", n_estimators=600, max_depth=3,
                                     learning_rate=0.03, subsample=0.7, random_state=42)
gb_q80   = GradientBoostingRegressor(loss="quantile", alpha=0.80, n_estimators=400, max_depth=3,
                                     learning_rate=0.03, subsample=0.7, random_state=42)

preds, trues, dates = [], [], []
for i in range(start_train, len(har)):
    tr = slice(0, i); te = slice(i, i+1)
    Xtr, ytr = har.iloc[tr][X_cols].to_numpy(), har.iloc[tr]["y"].to_numpy()
    Xte, yte = har.iloc[te][X_cols].to_numpy(), har.iloc[te]["y"].to_numpy()

    Xtr_s = scaler.fit_transform(Xtr)
    Xte_s = scaler.transform(Xte)

    gb_huber.fit(Xtr_s, ytr)
    gb_q80.fit(Xtr_s, ytr)
    yhat = 0.7*gb_huber.predict(Xte_s) + 0.3*gb_q80.predict(Xte_s)

    preds.append(float(np.exp(yhat[0])))
    trues.append(float(np.exp(yte[0])))
    dates.append(har.index[i])

preds, trues = np.array(preds), np.array(trues)
rmse = float(np.sqrt(mean_squared_error(trues, preds)))
eps = 1e-12
qlike = float(np.mean(np.log(np.clip(preds, eps, None)) + trues/np.clip(preds, eps, None)))
print(f"RV forecast — RMSE: {rmse:.6f} | QLIKE: {qlike:.6f} | steps: {len(preds)}")

# ---------------
# 6) Plot results
# ---------------
dfp = pd.DataFrame({"date_day": dates, "rv_true": trues, "rv_pred": preds}).set_index("date_day")
plt.figure(figsize=(10,4))
plt.plot(dfp.index, dfp["rv_true"], label="RV next (true)")
plt.plot(dfp.index, dfp["rv_pred"], label="RV next (pred)", alpha=0.9)
plt.legend(); plt.title("Next-day realized variance — HAR-X++ with fallback")
plt.tight_layout(); plt.show()

# ---------- 10) Save Artifacts ----------

# keep a safe copy of the scored posts; don't reuse `df` name later
POST_COLS = ["date","date_utc","text","text_clean","neg","neu","pos","sentiment","source"]
posts_scored = df.loc[:, [c for c in POST_COLS if c in df.columns]].copy()

stamp = pd.Timestamp.utcnow().strftime("%Y%m%d_%H%M")
posts_csv = os.path.join(drive_base, f"bitcoin_posts_scored_{stamp}.csv")
daily_csv = os.path.join(drive_base, f"bitcoin_sentiment_daily_{stamp}.csv")
joined_csv = os.path.join(drive_base, f"bitcoin_sentiment_joined_{stamp}.csv")
model_path = os.path.join(drive_base, f"nextday_direction_model_{stamp}.joblib")

# this will never KeyError even if some cols are missing
posts_scored.to_csv(posts_csv, index=False)
df_daily.to_csv(daily_csv, index=False)
data.to_csv(joined_csv, index=False)

import joblib
# pick whichever model exists
model_to_save = (
    locals().get("pipe_holdout")
    or locals().get("pipe")          # your earlier pipeline
    or locals().get("clf")           # from GB/HGB sections
)
if model_to_save is not None:
    joblib.dump(model_to_save, model_path)
else:
    print("⚠️ No model object found to save; skipping model dump.")

print("\nSaved artifacts to:")
print(posts_csv)
print(daily_csv)
print(joined_csv)
print(model_path)

# ===== 11) Price forecasts using sentiment (direction) + HAR (volatility) =====
import numpy as np, pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import math
np.set_printoptions(precision=4, suppress=True)

# ------------------------------------------------------------
# Inputs we expect from previous cells (with fallbacks if missing)
# ------------------------------------------------------------
# Spot price (use last Adj Close from your 'px' daily table if available)
if "px" in globals() and "Adj Close" in px.columns:
    spot = float(px["Adj Close"].dropna().iloc[-1])
    last_day = pd.to_datetime(px["date_day"].iloc[-1])
else:
    # fallback: daily 'data' table
    spot = float(data["Adj Close"].dropna().iloc[-1])
    last_day = pd.to_datetime(data["date_day"].dropna().iloc[-1])

# ---------- Direction model (daily sentiment + price features) ----------
import numpy as np, pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.calibration import CalibratedClassifierCV

fe_dir = data.copy()

# Strictly lagged, yesterday-only features
fe_dir["sent_lag1"]  = fe_dir["sent_balance"].shift(1)
fe_dir["sent_lag3"]  = fe_dir["sent_balance"].shift(3)
fe_dir["sent_r3"]    = fe_dir["sent_balance"].shift(1).rolling(3, min_periods=2).mean()
fe_dir["sent_r7"]    = fe_dir["sent_balance"].shift(1).rolling(7, min_periods=3).mean()
fe_dir["ret_lag1"]   = fe_dir["ret"].shift(1)
fe_dir["ret_lag3"]   = fe_dir["ret"].shift(3)
fe_dir["vol_5"]      = fe_dir["ret"].shift(1).rolling(5,  min_periods=3).std()
fe_dir["vol_10"]     = fe_dir["ret"].shift(1).rolling(10, min_periods=5).std()

# Per‑source balances if available
for c in [col for col in fe_dir.columns if col.startswith("pos_")]:
    neg_c = "neg_" + c[4:]
    if neg_c in fe_dir.columns:
        bal = fe_dir[c] - fe_dir[neg_c]
        fe_dir[c.replace("pos_","srcbal_") + "_lag1"] = bal.shift(1)
        fe_dir[c.replace("pos_","srcbal_") + "_r3"]   = bal.shift(1).rolling(3, min_periods=2).mean()

# Keep target rows
fe_dir = fe_dir.dropna(subset=["up_next"]).copy()

feat_cols_dir = [c for c in fe_dir.columns
                 if any(c.startswith(p) for p in ("sent_","ret_","vol_","srcbal_"))]
feat_cols_dir = [c for c in feat_cols_dir if fe_dir[c].notna().any()]

imp_dir = SimpleImputer(strategy="mean")
dir_model_used = "heuristic"   # default
p_up_1d = 0.5                   # default

# Build the "tomorrow" row with same features
tmp = data.copy()
tmp["sent_lag1"]  = tmp["sent_balance"].shift(1)
tmp["sent_lag3"]  = tmp["sent_balance"].shift(3)
tmp["sent_r3"]    = tmp["sent_balance"].shift(1).rolling(3, min_periods=2).mean()
tmp["sent_r7"]    = tmp["sent_balance"].shift(1).rolling(7, min_periods=3).mean()
tmp["ret_lag1"]   = tmp["ret"].shift(1)
tmp["ret_lag3"]   = tmp["ret"].shift(3)
tmp["vol_5"]      = tmp["ret"].shift(1).rolling(5,  min_periods=3).std()
tmp["vol_10"]     = tmp["ret"].shift(1).rolling(10, min_periods=5).std()
for c in [col for col in tmp.columns if col.startswith("pos_")]:
    neg_c = "neg_" + c[4:]
    if neg_c in tmp.columns:
        bal = tmp[c] - tmp[neg_c]
        tmp[c.replace("pos_","srcbal_") + "_lag1"] = bal.shift(1)
        tmp[c.replace("pos_","srcbal_") + "_r3"]   = bal.shift(1).rolling(3, min_periods=2).mean()

x_next_dir = tmp.iloc[[-1]][feat_cols_dir] if feat_cols_dir else None

enough_rows  = len(fe_dir) >= 20
two_classes  = fe_dir["up_next"].nunique() >= 2
have_feats   = bool(feat_cols_dir)

if enough_rows and two_classes and have_feats:
    X_all = imp_dir.fit_transform(fe_dir[feat_cols_dir])
    y_all = fe_dir["up_next"].astype(int).to_numpy()

    # Choose model by data size
    if len(fe_dir) >= 60:
        base = HistGradientBoostingClassifier(
            max_depth=3, learning_rate=0.08, max_iter=400,
            min_samples_leaf=5, random_state=42
        )
        mdl = CalibratedClassifierCV(base, method="isotonic",
                                     cv=min(5, max(2, len(y_all)//5)))
        dir_model_used = "HGB+isotonic"
    else:
        mdl = LogisticRegression(max_iter=1000, class_weight="balanced")
        dir_model_used = "Logit(balanced)"

    mdl.fit(X_all, y_all)
    p_raw = float(mdl.predict_proba(imp_dir.transform(x_next_dir))[:, 1])
    lo, hi = (0.05, 0.95) if len(fe_dir) < 60 else (0.02, 0.98)
    p_up_1d = float(np.clip(p_raw, lo, hi))
else:
    # Tiny-mode heuristic to keep pipeline running
    s1 = float(data["sent_balance"].shift(1).iloc[-1]) if "sent_balance" in data.columns else 0.0
    r1 = float(data["ret"].shift(1).iloc[-1]) if "ret" in data.columns else 0.0
    s_std = float(data["sent_balance"].shift(1).std() or 1.0) if "sent_balance" in data.columns else 1.0
    r_std = float(data["ret"].shift(1).std() or 1.0) if "ret" in data.columns else 1.0
    z_s = 0.0 if s_std == 0 or np.isnan(s_std) else np.tanh(s1 / (s_std + 1e-9))
    z_r = 0.0 if r_std == 0 or np.isnan(r_std) else np.tanh(r1 / (r_std + 1e-9))
    p_up_1d = float(np.clip(0.5 + 0.20*z_s + 0.10*z_r, 0.05, 0.95))
    dir_model_used = "heuristic"

print(f"Direction P(up) via {dir_model_used}: {p_up_1d:.3f}  "
      f"(rows={len(fe_dir)}, feats={len(feat_cols_dir)}, classes={fe_dir['up_next'].nunique() if 'up_next' in fe_dir else 0})")


# ---------- Volatility model (HAR‑X++) for next‑day variance ----------
# We expect 'har' & 'X_cols' from your HAR section. If not present, rebuild minimal HAR features from rv_daily.
need_rebuild_har = not ("har" in globals() and "X_cols" in globals() and len(har) >= 25)
if need_rebuild_har:
    # Minimal rebuild from earlier artifacts available in your notebook:
    # Use rv_daily already computed in the ccxt block; add EMA + range lags only.
    rvx = rv_daily.copy()
    if "rv_range" not in rvx.columns:
        rvx["rv_range"] = rvx["rv"]   # fallback
    rvx["rv_ema5"]   = rvx["rv"].ewm(span=5,  min_periods=5).mean()
    rvx["rv_ema22"]  = rvx["rv"].ewm(span=22, min_periods=22).mean()
    rvx["rv_d_lag1"] = rvx["rv"].shift(1)
    rvx["rv_w_lag1"] = rvx["rv_ema5"].shift(1)
    rvx["rv_m_lag1"] = rvx["rv_ema22"].shift(1)
    rvx["rv_std5_lag1"]  = rvx["rv"].rolling(5, min_periods=5).std().shift(1)
    rvx["rv_range_lag1"] = rvx["rv_range"].shift(1)
    har = rvx.copy()
    har["y"] = np.log(har["rv_next"].clip(lower=1e-12))
    X_cols = ["rv_d_lag1","rv_w_lag1","rv_m_lag1","rv_std5_lag1","rv_range_lag1"]
    har = har.dropna(subset=X_cols + ["y"])

scaler_vol = RobustScaler()
gb_huber = GradientBoostingRegressor(
    loss="huber", n_estimators=600, max_depth=3,
    learning_rate=0.03, subsample=0.7, random_state=42
)
gb_q80   = GradientBoostingRegressor(
    loss="quantile", alpha=0.80, n_estimators=400, max_depth=3,
    learning_rate=0.03, subsample=0.7, random_state=42
)

# Fit on all rows, predict the next day (use last available row features)
Xv = har[X_cols].to_numpy()
yv = har["y"].to_numpy()
Xv_s = scaler_vol.fit_transform(Xv)
gb_huber.fit(Xv_s, yv)
gb_q80.fit(Xv_s, yv)

x_last = har.iloc[[-1]][X_cols].to_numpy()
rv_pred_next = float(np.exp(0.7*gb_huber.predict(scaler_vol.transform(x_last))[0] +
                            0.3*gb_q80.predict(scaler_vol.transform(x_last))[0]))  # daily RV

sigma_1d = math.sqrt(rv_pred_next)   # daily volatility (std of log-returns)

# ---------- Horizon scaling & drift assumption ----------
# Drift: combine recent average daily return with a small sentiment tilt.
mu_recent = float(data["ret"].dropna().tail(30).mean())  # 30-day mean daily return
sent_tilt  = 0.5 * (p_up_1d - 0.5)                       # scale factor (tweakable)
mu_1d = mu_recent + sent_tilt * sigma_1d                 # daily expected return

# Scale volatility to longer horizons (rough Brownian scaling)
def horizon_params(days):
    mu = mu_1d * days
    sigma = sigma_1d * math.sqrt(days)
    return mu, sigma

# ---------- Monte Carlo to get price distributions ----------
rng = np.random.default_rng(42)

def simulate_prices(S0, days, sims=20000):
    mu, sigma = horizon_params(days)
    # simulate one-step lognormal for the horizon
    r = rng.normal(loc=mu, scale=sigma, size=sims)
    ST = S0 * np.exp(r)
    return ST

def summarize(ST):
    pct = np.percentile
    return {
        "median": float(pct(ST, 50)),
        "p10":    float(pct(ST, 10)),
        "p90":    float(pct(ST, 90)),
        "p05":    float(pct(ST, 5)),
        "p95":    float(pct(ST, 95)),
        "mean":   float(np.mean(ST)),
        "prob_up": float(np.mean(ST > spot))
    }

res_1d = summarize(simulate_prices(spot, 1))
res_7d = summarize(simulate_prices(spot, 7))
res_30d= summarize(simulate_prices(spot, 30))

print("\n# ===== Price Forecasts (spot = {:.2f}, as of {}) =====".format(spot, last_day.date()))
print("Direction (P[Up] tomorrow): {:.1f}%".format(100*p_up_1d))
print("Daily vol (HAR): {:.2%} (sigma_1d), RV={:.6f}".format(sigma_1d, rv_pred_next))
print("\n1‑Day forecast:")
print("  Median: {:,.2f} | 10–90%: [{:,.2f}, {:,.2f}] | 5–95%: [{:,.2f}, {:,.2f}] | P(Up): {:.1f}%"
      .format(res_1d["median"], res_1d["p10"], res_1d["p90"], res_1d["p05"], res_1d["p95"], 100*res_1d["prob_up"]))
print("\n7‑Day forecast:")
print("  Median: {:,.2f} | 10–90%: [{:,.2f}, {:,.2f}] | 5–95%: [{:,.2f}, {:,.2f}] | P(Up): {:.1f}%"
      .format(res_7d["median"], res_7d["p10"], res_7d["p90"], res_7d["p05"], res_7d["p95"], 100*res_7d["prob_up"]))
print("\n30‑Day forecast:")
print("  Median: {:,.2f} | 10–90%: [{:,.2f}, {:,.2f}] | 5–95%: [{:,.2f}, {:,.2f}] | P(Up): {:.1f}%"
      .format(res_30d["median"], res_30d["p10"], res_30d["p90"], res_30d["p05"], res_30d["p95"], 100*res_30d["prob_up"]))

# ---------- ASSUMPTIONS you can tweak ----------
# 1) mu_recent window (default 30d)
# 2) sentiment tilt strength (0.5) -> 'sent_tilt'
# 3) Brownian scaling for horizon (sqrt time)
# 4) One-step horizon MC (no path dependence). For path MC, simulate daily steps and compound.
